# ë†€ì´ ìƒí˜¸ì‘ìš© ë¶„ì„ í”„ë¡œê·¸ë¨ ê°œë°œ ê°€ì´ë“œ

## ğŸš€ ê°œìš”
ë¶€ëª¨-ìë…€ ë†€ì´ ì˜ìƒì„ ë¶„ì„í•˜ì—¬ ìƒí˜¸ì‘ìš© ì§ˆì„ í‰ê°€í•˜ê³  ë°œë‹¬ ì§€ì› ìˆ˜ì¤€ì„ ì¸¡ì •í•˜ëŠ” AI ê¸°ë°˜ ë¶„ì„ ì‹œìŠ¤í…œ ê°œë°œ ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
1. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
2. [í•µì‹¬ ë¶„ì„ ëª¨ë“ˆ](#í•µì‹¬-ë¶„ì„-ëª¨ë“ˆ)
3. [ì ìˆ˜ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜](#ì ìˆ˜-ê³„ì‚°-ì•Œê³ ë¦¬ì¦˜)
4. [ë°ì´í„° í†µí•© ë° ë³´ê³ ì„œ](#ë°ì´í„°-í†µí•©-ë°-ë³´ê³ ì„œ)
5. [ì‹¤ì‹œê°„ ë¶„ì„](#ì‹¤ì‹œê°„-ë¶„ì„)
6. [API ì„¤ê³„](#api-ì„¤ê³„)
7. [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)

---

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ë””ë ‰í† ë¦¬ êµ¬ì¡°
```
play_interaction_analyzer/
â”œâ”€â”€ data_processing/
â”‚   â”œâ”€â”€ video_analyzer.py      # ë¹„ë””ì˜¤ ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ audio_analyzer.py      # ìŒì„± ì „ì‚¬ ë° ë¶„ì„
â”‚   â””â”€â”€ data_merger.py         # ë°ì´í„° í†µí•©
â”œâ”€â”€ analysis_modules/
â”‚   â”œâ”€â”€ physical_interaction.py
â”‚   â”œâ”€â”€ emotional_interaction.py
â”‚   â”œâ”€â”€ language_interaction.py
â”‚   â””â”€â”€ play_patterns.py
â”œâ”€â”€ scoring/
â”‚   â”œâ”€â”€ metrics_calculator.py
â”‚   â””â”€â”€ report_generator.py
â”œâ”€â”€ visualization/
â”‚   â””â”€â”€ dashboard_generator.py
â”œâ”€â”€ api/
â”‚   â””â”€â”€ endpoints.py
â””â”€â”€ config/
    â””â”€â”€ settings.py
```

---

## í•µì‹¬ ë¶„ì„ ëª¨ë“ˆ

### 1. ë¹„ë””ì˜¤ ë¶„ì„ ëª¨ë“ˆ

#### `video_analyzer.py`
```python
from google.cloud import videointelligence

class VideoAnalyzer:
    def __init__(self):
        self.client = videointelligence.VideoIntelligenceServiceClient()
        self.features = [
            videointelligence.Feature.OBJECT_TRACKING,
            videointelligence.Feature.FACE_DETECTION,
            videointelligence.Feature.PERSON_DETECTION,
            videointelligence.Feature.SHOT_CHANGE_DETECTION
        ]
    
    def analyze_video(self, video_path):
        """
        ë¹„ë””ì˜¤ ë¶„ì„ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        
        Returns:
            dict: {
                'object_tracking': [...],
                'face_detection': [...],
                'person_detection': [...],
                'shot_changes': [...]
            }
        """
        with open(video_path, 'rb') as f:
            input_content = f.read()
        
        operation = self.client.annotate_video(
            request={
                "features": self.features,
                "input_content": input_content,
            }
        )
        
        result = operation.result(timeout=300)
        return self._process_results(result)
    
    def _process_results(self, result):
        """ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜"""
        return {
            'object_tracking': self._extract_object_tracks(result),
            'face_detection': self._extract_face_data(result),
            'person_detection': self._extract_person_data(result),
            'shot_changes': self._extract_shot_changes(result)
        }
```

### 2. ë¬¼ë¦¬ì  ìƒí˜¸ì‘ìš© ë¶„ì„

#### `physical_interaction.py`
```python
import numpy as np
from typing import List, Dict, Tuple

class PhysicalInteractionAnalyzer:
    def __init__(self):
        self.proximity_threshold = 0.3  # ê·¼ì ‘ íŒë‹¨ ê¸°ì¤€
        self.sync_window = 2.0  # ë™ê¸°í™” íŒë‹¨ ì‹œê°„ ì°½(ì´ˆ)
    
    def calculate_proximity(self, person1_bbox: Dict, person2_bbox: Dict) -> float:
        """
        ë‘ ì‚¬ëŒ ê°„ ì •ê·œí™”ëœ ê±°ë¦¬ ê³„ì‚°
        
        Args:
            person1_bbox: {'left': 0.1, 'top': 0.2, 'right': 0.3, 'bottom': 0.8}
            person2_bbox: {'left': 0.4, 'top': 0.2, 'right': 0.6, 'bottom': 0.8}
        
        Returns:
            float: 0-1 ì‚¬ì´ì˜ ê±°ë¦¬ ê°’ (0=ë§¤ìš° ê°€ê¹Œì›€, 1=ë§¤ìš° ë©€ìŒ)
        """
        # ì¤‘ì‹¬ì  ê³„ì‚°
        center1 = self._get_center(person1_bbox)
        center2 = self._get_center(person2_bbox)
        
        # ìœ í´ë¦¬ë“œ ê±°ë¦¬
        distance = np.sqrt((center1[0] - center2[0])**2 + 
                          (center1[1] - center2[1])**2)
        
        # ì •ê·œí™” (ëŒ€ê°ì„  ê¸¸ì´ ê¸°ì¤€)
        normalized_distance = distance / np.sqrt(2)
        return min(normalized_distance, 1.0)
    
    def analyze_movement_synchrony(self, movements1: List, movements2: List) -> Dict:
        """
        ì›€ì§ì„ ë™ê¸°í™” ë¶„ì„
        
        Returns:
            dict: {
                'sync_score': float,  # 0-1
                'synchronized_events': List[Tuple[time, type]],
                'mirroring_count': int
            }
        """
        sync_events = []
        
        for m1 in movements1:
            for m2 in movements2:
                time_diff = abs(m1['time'] - m2['time'])
                if time_diff <= self.sync_window:
                    if self._is_similar_movement(m1, m2):
                        sync_events.append((m1['time'], 'synchronized'))
                    elif self._is_mirrored_movement(m1, m2):
                        sync_events.append((m1['time'], 'mirrored'))
        
        return {
            'sync_score': len(sync_events) / max(len(movements1), len(movements2)),
            'synchronized_events': sync_events,
            'mirroring_count': sum(1 for _, t in sync_events if t == 'mirrored')
        }
    
    def calculate_activity_metrics(self, bbox_sequence: List) -> Dict:
        """
        í™œë™ì„± ìˆ˜ì¤€ ì¸¡ì •
        
        Returns:
            dict: {
                'activity_level': str,  # 'low', 'medium', 'high'
                'movement_speed': float,
                'activity_area': float,
                'static_ratio': float
            }
        """
        if len(bbox_sequence) < 2:
            return self._default_activity_metrics()
        
        # ì›€ì§ì„ ì†ë„ ê³„ì‚°
        speeds = []
        for i in range(1, len(bbox_sequence)):
            speed = self._calculate_speed(
                bbox_sequence[i-1], 
                bbox_sequence[i]
            )
            speeds.append(speed)
        
        avg_speed = np.mean(speeds)
        
        # í™œë™ ì˜ì—­ ê³„ì‚°
        all_centers = [self._get_center(bbox) for bbox in bbox_sequence]
        activity_area = self._calculate_coverage_area(all_centers)
        
        # ì •ì  ì‹œê°„ ë¹„ìœ¨
        static_frames = sum(1 for s in speeds if s < 0.01)
        static_ratio = static_frames / len(speeds)
        
        # í™œë™ ìˆ˜ì¤€ íŒì •
        if avg_speed < 0.05 and static_ratio > 0.7:
            activity_level = 'low'
        elif avg_speed > 0.15 or static_ratio < 0.3:
            activity_level = 'high'
        else:
            activity_level = 'medium'
        
        return {
            'activity_level': activity_level,
            'movement_speed': avg_speed,
            'activity_area': activity_area,
            'static_ratio': static_ratio
        }
    
    def _get_center(self, bbox: Dict) -> Tuple[float, float]:
        """ë°”ìš´ë”© ë°•ìŠ¤ì˜ ì¤‘ì‹¬ì  ê³„ì‚°"""
        x = (bbox['left'] + bbox['right']) / 2
        y = (bbox['top'] + bbox['bottom']) / 2
        return (x, y)
    
    def _calculate_speed(self, bbox1: Dict, bbox2: Dict) -> float:
        """ë‘ í”„ë ˆì„ ê°„ ì›€ì§ì„ ì†ë„ ê³„ì‚°"""
        center1 = self._get_center(bbox1)
        center2 = self._get_center(bbox2)
        distance = np.sqrt((center1[0] - center2[0])**2 + 
                          (center1[1] - center2[1])**2)
        return distance
    
    def _calculate_coverage_area(self, centers: List[Tuple]) -> float:
        """í™œë™ ì˜ì—­ì˜ ë©´ì  ê³„ì‚°"""
        if len(centers) < 3:
            return 0.0
        
        xs = [c[0] for c in centers]
        ys = [c[1] for c in centers]
        
        area = (max(xs) - min(xs)) * (max(ys) - min(ys))
        return area
```

### 3. ì–¸ì–´ ìƒí˜¸ì‘ìš© ë¶„ì„

#### `language_interaction.py`
```python
import re
from collections import Counter
from typing import List, Dict
import openai

class LanguageInteractionAnalyzer:
    def __init__(self, api_key: str):
        openai.api_key = api_key
        self.analysis_prompt = """
        ìŒì„± ì „ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì¶”ì¶œí•˜ì„¸ìš”:

        1. ë°œí™” í†µê³„:
           - ê° í™”ìë³„ ë°œí™” íšŸìˆ˜
           - í‰ê·  ë°œí™” ê¸¸ì´ (ë‹¨ì–´ ìˆ˜)
           - ë°œí™” ê°„ê²© (ì´ˆ)

        2. ë°œí™” ìœ í˜• ë¶„ë¥˜:
           - ì§ˆë¬¸: "ì–´ë–»ê²Œ", "ë­", "ì™œ", "ë­˜", "ì–´ë””" ë“±ìœ¼ë¡œ ì‹œì‘
           - ì§€ì‹œ/ì œì•ˆ: "í•´ë´", "í•˜ì", "í•´ì¤„ê²Œ", "í•´ì•¼", "í•˜ë©´" ë“± í¬í•¨
           - ê°ì • í‘œí˜„: ê°íƒ„ì‚¬, ê°ì • ë‹¨ì–´ ("ì¢‹ì•„", "ì‹«ì–´", "ì˜ˆì˜ë‹¤" ë“±)
           - ì¹­ì°¬/ê²©ë ¤: "ì˜í–ˆì–´", "ë©‹ì§€ë‹¤", "ëŒ€ë‹¨í•´" ë“±

        3. ìƒí˜¸ì‘ìš© íŒ¨í„´:
           - ëŒ€í™” ì£¼ë„ì„± (ë°œí™” ì‹œì‘ ë¹ˆë„)
           - ë°˜ì‘ ì‹œê°„ (í‰ê·  ì‘ë‹µ ê°„ê²©)
           - ëŒ€í™” ì—°ê²°ì„± (ì£¼ì œ ì¼ê´€ì„± ì ìˆ˜ 0-1)

        4. ì£¼ìš” í‚¤ì›Œë“œ:
           - ë¹ˆë„ìˆ˜ ìƒìœ„ 10ê°œ ë‹¨ì–´
           - ë†€ì´ ê´€ë ¨ ì–´íœ˜
           - í˜¸ëª… ë¹ˆë„

        5. ë°œë‹¬ ì§€í‘œ:
           - ë¬¸ì¥ ë³µì¡ë„
           - ì–´íœ˜ ë‹¤ì–‘ì„±
           - ìƒí˜¸ì‘ìš© ì–¸ì–´ ì‚¬ìš©

        ì¶œë ¥ í˜•ì‹: JSON
        """
    
    def analyze_transcript(self, transcript: List[Dict]) -> Dict:
        """
        ì „ì‚¬ í…ìŠ¤íŠ¸ ë¶„ì„
        
        Args:
            transcript: [
                {'speaker': 'ì°¸ì„ì1', 'time': 5, 'text': 'ì–´ë–»ê²Œ í•˜ëŠ” ê±¸ ê±° ê°™ì•„.'},
                ...
            ]
        
        Returns:
            dict: ë¶„ì„ ê²°ê³¼
        """
        # ê¸°ë³¸ í†µê³„ ê³„ì‚°
        basic_stats = self._calculate_basic_stats(transcript)
        
        # GPTë¥¼ ì‚¬ìš©í•œ ì‹¬í™” ë¶„ì„
        detailed_analysis = self._gpt_analysis(transcript)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_keywords(transcript)
        
        # ëŒ€í™” íŒ¨í„´ ë¶„ì„
        conversation_patterns = self._analyze_conversation_patterns(transcript)
        
        return {
            'basic_stats': basic_stats,
            'detailed_analysis': detailed_analysis,
            'keywords': keywords,
            'conversation_patterns': conversation_patterns
        }
    
    def _calculate_basic_stats(self, transcript: List[Dict]) -> Dict:
        """ê¸°ë³¸ ë°œí™” í†µê³„ ê³„ì‚°"""
        stats = {}
        
        # í™”ìë³„ ê·¸ë£¹í™”
        speakers = {}
        for entry in transcript:
            speaker = entry['speaker']
            if speaker not in speakers:
                speakers[speaker] = []
            speakers[speaker].append(entry)
        
        # í™”ìë³„ í†µê³„
        for speaker, entries in speakers.items():
            utterance_count = len(entries)
            
            # í‰ê·  ë°œí™” ê¸¸ì´
            word_counts = [len(e['text'].split()) for e in entries]
            avg_length = sum(word_counts) / len(word_counts) if word_counts else 0
            
            # ë°œí™” ê°„ê²©
            intervals = []
            for i in range(1, len(entries)):
                interval = entries[i]['time'] - entries[i-1]['time']
                intervals.append(interval)
            avg_interval = sum(intervals) / len(intervals) if intervals else 0
            
            stats[speaker] = {
                'utterance_count': utterance_count,
                'avg_word_count': round(avg_length, 1),
                'avg_interval': round(avg_interval, 1),
                'total_words': sum(word_counts)
            }
        
        return stats
    
    def _gpt_analysis(self, transcript: List[Dict]) -> Dict:
        """GPTë¥¼ ì‚¬ìš©í•œ ì‹¬í™” ë¶„ì„"""
        # ì „ì‚¬ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        transcript_text = "\n".join([
            f"{entry['speaker']} ({entry['time']}ì´ˆ): {entry['text']}"
            for entry in transcript
        ])
        
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": self.analysis_prompt},
                {"role": "user", "content": transcript_text}
            ],
            temperature=0.3
        )
        
        # JSON íŒŒì‹±
        import json
        return json.loads(response.choices[0].message.content)
    
    def _extract_keywords(self, transcript: List[Dict]) -> Dict:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_words = []
        for entry in transcript:
            # ê¸°ë³¸ ì „ì²˜ë¦¬
            words = re.findall(r'\w+', entry['text'].lower())
            all_words.extend(words)
        
        # ë¶ˆìš©ì–´ ì œê±° (ê°„ë‹¨í•œ í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸)
        stopwords = {'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì—', 'ì˜'}
        filtered_words = [w for w in all_words if w not in stopwords and len(w) > 1]
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        word_freq = Counter(filtered_words)
        
        return {
            'top_keywords': word_freq.most_common(10),
            'total_unique_words': len(set(filtered_words)),
            'total_words': len(filtered_words)
        }
    
    def _analyze_conversation_patterns(self, transcript: List[Dict]) -> Dict:
        """ëŒ€í™” íŒ¨í„´ ë¶„ì„"""
        if len(transcript) < 2:
            return {}
        
        patterns = {
            'turn_taking': [],
            'response_times': [],
            'initiation_count': {}
        }
        
        # í„´í…Œì´í‚¹ ë¶„ì„
        for i in range(1, len(transcript)):
            if transcript[i]['speaker'] != transcript[i-1]['speaker']:
                response_time = transcript[i]['time'] - transcript[i-1]['time']
                patterns['response_times'].append(response_time)
                patterns['turn_taking'].append({
                    'from': transcript[i-1]['speaker'],
                    'to': transcript[i]['speaker'],
                    'time': transcript[i]['time'],
                    'response_time': response_time
                })
        
        # ëŒ€í™” ì‹œì‘ íšŸìˆ˜
        for i, entry in enumerate(transcript):
            if i == 0 or transcript[i]['time'] - transcript[i-1]['time'] > 3.0:
                speaker = entry['speaker']
                patterns['initiation_count'][speaker] = patterns['initiation_count'].get(speaker, 0) + 1
        
        # í‰ê·  ë°˜ì‘ ì‹œê°„
        avg_response_time = sum(patterns['response_times']) / len(patterns['response_times']) if patterns['response_times'] else 0
        
        return {
            'avg_response_time': round(avg_response_time, 2),
            'turn_count': len(patterns['turn_taking']),
            'initiation_count': patterns['initiation_count'],
            'conversation_flow': patterns['turn_taking'][:10]  # ì²˜ìŒ 10ê°œë§Œ
        }
```

### 4. ê°ì •ì  ìƒí˜¸ì‘ìš© ë¶„ì„

#### `emotional_interaction.py`
```python
import numpy as np
from typing import List, Dict, Tuple

class EmotionalInteractionAnalyzer:
    def __init__(self):
        self.face_size_threshold = 0.1  # ì–¼êµ´ í¬ê¸° ì„ê³„ê°’
        self.gaze_alignment_threshold = 0.8  # ì‹œì„  ì •ë ¬ ì„ê³„ê°’
    
    def analyze_face_orientation(self, face_data: List[Dict]) -> Dict:
        """
        ì–¼êµ´ ì§€í–¥ í–‰ë™ ë¶„ì„
        
        Args:
            face_data: ì–¼êµ´ ê°ì§€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        
        Returns:
            dict: {
                'mutual_gaze_time': float,  # ìƒí˜¸ ì‘ì‹œ ì‹œê°„ ë¹„ìœ¨
                'face_to_face_ratio': float,  # ì–¼êµ´ ëŒ€ë©´ ë¹„ìœ¨
                'proximity_changes': List[Dict],  # ê·¼ì ‘ì„± ë³€í™”
                'engagement_score': float  # 0-1
            }
        """
        if not face_data:
            return self._default_face_metrics()
        
        mutual_gaze_frames = 0
        face_to_face_frames = 0
        proximity_changes = []
        
        for i, frame in enumerate(face_data):
            if len(frame.get('faces', [])) >= 2:
                # ë‘ ì–¼êµ´ì´ ëª¨ë‘ ê°ì§€ëœ ê²½ìš°
                face1, face2 = frame['faces'][:2]
                
                # ìƒí˜¸ ì‘ì‹œ íŒë‹¨
                if self._is_mutual_gaze(face1, face2):
                    mutual_gaze_frames += 1
                
                # ì–¼êµ´ ëŒ€ë©´ íŒë‹¨
                if self._is_face_to_face(face1, face2):
                    face_to_face_frames += 1
                
                # ê·¼ì ‘ì„± ë³€í™” ì¶”ì 
                if i > 0 and len(face_data[i-1].get('faces', [])) >= 2:
                    prev_distance = self._calculate_face_distance(
                        face_data[i-1]['faces'][0], 
                        face_data[i-1]['faces'][1]
                    )
                    curr_distance = self._calculate_face_distance(face1, face2)
                    
                    if abs(curr_distance - prev_distance) > 0.05:
                        proximity_changes.append({
                            'time': frame['time'],
                            'change': 'closer' if curr_distance < prev_distance else 'farther',
                            'magnitude': abs(curr_distance - prev_distance)
                        })
        
        total_frames = len(face_data)
        
        return {
            'mutual_gaze_time': mutual_gaze_frames / total_frames if total_frames > 0 else 0,
            'face_to_face_ratio': face_to_face_frames / total_frames if total_frames > 0 else 0,
            'proximity_changes': proximity_changes,
            'engagement_score': self._calculate_engagement_score(
                mutual_gaze_frames / total_frames if total_frames > 0 else 0,
                face_to_face_frames / total_frames if total_frames > 0 else 0,
                len(proximity_changes)
            )
        }
    
    def estimate_emotional_states(self, face_data: List[Dict], movement_data: List[Dict]) -> Dict:
        """
        ê°ì • ìƒíƒœ ì¶”ì • (í‘œì • ì¸ì‹ ì—†ì´)
        
        Returns:
            dict: {
                'engagement_periods': List[Tuple[start, end, level]],
                'interaction_quality': str,  # 'high', 'medium', 'low'
                'emotional_synchrony': float  # 0-1
            }
        """
        engagement_periods = []
        current_period = None
        
        for i, frame in enumerate(face_data):
            engagement_level = self._estimate_frame_engagement(frame, movement_data, i)
            
            if engagement_level > 0.6:  # ë†’ì€ ì°¸ì—¬ë„
                if current_period is None:
                    current_period = [frame['time'], frame['time'], 'high']
                else:
                    current_period[1] = frame['time']
            else:
                if current_period is not None:
                    engagement_periods.append(tuple(current_period))
                    current_period = None
        
        # ë§ˆì§€ë§‰ ê¸°ê°„ ì¶”ê°€
        if current_period is not None:
            engagement_periods.append(tuple(current_period))
        
        # ìƒí˜¸ì‘ìš© í’ˆì§ˆ íŒë‹¨
        total_high_engagement_time = sum(end - start for start, end, _ in engagement_periods)
        total_time = face_data[-1]['time'] - face_data[0]['time'] if face_data else 1
        engagement_ratio = total_high_engagement_time / total_time
        
        if engagement_ratio > 0.6:
            interaction_quality = 'high'
        elif engagement_ratio > 0.3:
            interaction_quality = 'medium'
        else:
            interaction_quality = 'low'
        
        return {
            'engagement_periods': engagement_periods,
            'interaction_quality': interaction_quality,
            'emotional_synchrony': self._calculate_emotional_synchrony(face_data, movement_data)
        }
    
    def _is_mutual_gaze(self, face1: Dict, face2: Dict) -> bool:
        """ìƒí˜¸ ì‘ì‹œ ì—¬ë¶€ íŒë‹¨"""
        # ì–¼êµ´ ì¤‘ì‹¬ì  ê³„ì‚°
        center1 = self._get_face_center(face1)
        center2 = self._get_face_center(face2)
        
        # ì–¼êµ´ ë°©í–¥ ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
        # ì‹¤ì œë¡œëŠ” ëœë“œë§ˆí¬ ê¸°ë°˜ ê³„ì‚°ì´ í•„ìš”í•˜ì§€ë§Œ, bboxë§Œìœ¼ë¡œ ì¶”ì •
        horizontal_alignment = abs(center1[1] - center2[1]) < 0.2
        facing_each_other = abs(center1[0] - center2[0]) > 0.1
        
        return horizontal_alignment and facing_each_other
    
    def _is_face_to_face(self, face1: Dict, face2: Dict) -> bool:
        """ì–¼êµ´ ëŒ€ë©´ ì—¬ë¶€ íŒë‹¨"""
        center1 = self._get_face_center(face1)
        center2 = self._get_face_center(face2)
        
        # ìˆ˜í‰ ì •ë ¬ í™•ì¸
        horizontal_alignment = abs(center1[1] - center2[1]) < 0.3
        
        # ì ì ˆí•œ ê±°ë¦¬ í™•ì¸
        distance = abs(center1[0] - center2[0])
        appropriate_distance = 0.2 < distance < 0.6
        
        return horizontal_alignment and appropriate_distance
    
    def _calculate_face_distance(self, face1: Dict, face2: Dict) -> float:
        """ë‘ ì–¼êµ´ ê°„ ê±°ë¦¬ ê³„ì‚°"""
        center1 = self._get_face_center(face1)
        center2 = self._get_face_center(face2)
        
        distance = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
        return distance
    
    def _get_face_center(self, face: Dict) -> Tuple[float, float]:
        """ì–¼êµ´ ì¤‘ì‹¬ì  ê³„ì‚°"""
        bbox = face['boundingBox']
        x = (bbox['left'] + bbox['right']) / 2
        y = (bbox['top'] + bbox['bottom']) / 2
        return (x, y)
    
    def _calculate_engagement_score(self, gaze_ratio: float, face_ratio: float, changes: int) -> float:
        """ì°¸ì—¬ë„ ì ìˆ˜ ê³„ì‚°"""
        # ê°€ì¤‘ì¹˜ ì ìš©
        score = (gaze_ratio * 0.4 + face_ratio * 0.4 + min(changes / 10, 1) * 0.2)
        return round(score, 2)
    
    def _estimate_frame_engagement(self, face_frame: Dict, movement_data: List[Dict], frame_idx: int) -> float:
        """í”„ë ˆì„ë³„ ì°¸ì—¬ë„ ì¶”ì •"""
        engagement = 0.0
        
        # ì–¼êµ´ ê°ì§€ ì—¬ë¶€
        if face_frame.get('faces'):
            engagement += 0.3
            
            # ì–¼êµ´ í¬ê¸° (ê·¼ì ‘ì„± ì§€í‘œ)
            if len(face_frame['faces']) >= 2:
                avg_face_size = np.mean([self._get_face_size(f) for f in face_frame['faces'][:2]])
                if avg_face_size > self.face_size_threshold:
                    engagement += 0.3
        
        # ì›€ì§ì„ í™œë™ì„±
        if frame_idx < len(movement_data):
            movement = movement_data[frame_idx]
            if movement.get('activity_level') in ['medium', 'high']:
                engagement += 0.4
        
        return engagement
    
    def _get_face_size(self, face: Dict) -> float:
        """ì–¼êµ´ í¬ê¸° ê³„ì‚°"""
        bbox = face['boundingBox']
        width = bbox['right'] - bbox['left']
        height = bbox['bottom'] - bbox['top']
        return width * height
    
    def _calculate_emotional_synchrony(self, face_data: List[Dict], movement_data: List[Dict]) -> float:
        """ê°ì •ì  ë™ê¸°í™” ê³„ì‚°"""
        if not face_data or not movement_data:
            return 0.0
        
        sync_frames = 0
        total_frames = min(len(face_data), len(movement_data))
        
        for i in range(total_frames):
            face_engagement = self._estimate_frame_engagement(face_data[i], movement_data, i)
            
            # ë‘ ì°¸ê°€ìê°€ ë¹„ìŠ·í•œ ì°¸ì—¬ë„ë¥¼ ë³´ì´ëŠ” ê²½ìš°
            if face_engagement > 0.5 and len(face_data[i].get('faces', [])) >= 2:
                sync_frames += 1
        
        return sync_frames / total_frames if total_frames > 0 else 0.0
    
    def _default_face_metrics(self) -> Dict:
        """ê¸°ë³¸ ì–¼êµ´ ì§€í‘œ"""
        return {
            'mutual_gaze_time': 0.0,
            'face_to_face_ratio': 0.0,
            'proximity_changes': [],
            'engagement_score': 0.0
        }
```

### 5. ë†€ì´ íŒ¨í„´ ë¶„ì„

#### `play_patterns.py`
```python
from typing import List, Dict, Tuple
import numpy as np

class PlayPatternAnalyzer:
    def __init__(self):
        self.min_activity_duration = 10  # ìµœì†Œ í™œë™ ì§€ì† ì‹œê°„(ì´ˆ)
        self.transition_threshold = 0.3  # í™œë™ ì „í™˜ ì„ê³„ê°’
    
    def analyze_toy_usage(self, object_tracks: List[Dict]) -> Dict:
        """
        ì¥ë‚œê° ì‚¬ìš© íŒ¨í„´ ë¶„ì„
        
        Returns:
            dict: {
                'toys_detected': List[str],
                'usage_duration': Dict[str, float],
                'sharing_ratio': float,
                'toy_transitions': List[Dict]
            }
        """
        toys = {}
        transitions = []
        
        # ì¥ë‚œê°ë³„ ì‚¬ìš© ì‹œê°„ ê³„ì‚°
        for track in object_tracks:
            if track['category'] == 'toy':
                toy_id = track['entity_id']
                if toy_id not in toys:
                    toys[toy_id] = {
                        'first_seen': track['time'],
                        'last_seen': track['time'],
                        'frames': 1,
                        'shared_frames': 0
                    }
                else:
                    toys[toy_id]['last_seen'] = track['time']
                    toys[toy_id]['frames'] += 1
                
                # ê³µìœ  ì—¬ë¶€ í™•ì¸ (ë‘ ì‚¬ëŒì´ ë™ì‹œì— ìƒí˜¸ì‘ìš©)
                if self._is_shared_interaction(track):
                    toys[toy_id]['shared_frames'] += 1
        
        # ì‚¬ìš© ì‹œê°„ ë° ê³µìœ  ë¹„ìœ¨ ê³„ì‚°
        usage_duration = {}
        total_shared_frames = 0
        total_frames = 0
        
        for toy_id, data in toys.items():
            duration = data['last_seen'] - data['first_seen']
            usage_duration[toy_id] = duration
            total_shared_frames += data['shared_frames']
            total_frames += data['frames']
        
        sharing_ratio = total_shared_frames / total_frames if total_frames > 0 else 0
        
        # ì¥ë‚œê° ì „í™˜ ë¶„ì„
        prev_toy = None
        prev_time = None
        
        for track in sorted(object_tracks, key=lambda x: x['