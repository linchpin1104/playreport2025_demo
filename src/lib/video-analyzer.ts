import { readFileSync } from 'fs';
import { VideoIntelligenceServiceClient, protos } from '@google-cloud/video-intelligence';
import config from '@/lib/config';
import { VideoIntelligenceResults } from '@/types';

export interface VideoAnalysisOptions {
  enableVoiceAnalysis?: boolean;
  enableGestureRecognition?: boolean;
  enableObjectDetection?: boolean;
  enableFaceDetection?: boolean;
  enableTranscription?: boolean;
  enableSpeakerDiarization?: boolean;
  enableSentimentAnalysis?: boolean;
  enableQualityMetrics?: boolean;
  enableComprehensiveAnalysis?: boolean;
}

export class VideoAnalyzer {
  private readonly client: VideoIntelligenceServiceClient;
  private readonly features: protos.google.cloud.videointelligence.v1.Feature[];

  constructor() {
    this.client = new VideoIntelligenceServiceClient({
      projectId: config.googleCloud.projectId,
      keyFilename: config.googleCloud.keyFile,
    });

    this.features = [
      protos.google.cloud.videointelligence.v1.Feature.OBJECT_TRACKING,
      protos.google.cloud.videointelligence.v1.Feature.FACE_DETECTION,
      protos.google.cloud.videointelligence.v1.Feature.PERSON_DETECTION,
      protos.google.cloud.videointelligence.v1.Feature.SHOT_CHANGE_DETECTION,
      protos.google.cloud.videointelligence.v1.Feature.SPEECH_TRANSCRIPTION,
      protos.google.cloud.videointelligence.v1.Feature.TEXT_DETECTION,
    ];
  }

  /**
   * ÎπÑÎîîÏò§ Î∂ÑÏÑù Î∞è Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
   */
  async analyzeVideo(
    videoInput: string | Buffer, 
    options: VideoAnalysisOptions = {}
  ): Promise<VideoIntelligenceResults> {
    try {
      let request: any;

      // ÏûÖÎ†• ÌÉÄÏûÖÏóê Îî∞Îùº ÏöîÏ≤≠ Íµ¨ÏÑ±
      if (typeof videoInput === 'string') {
        // GCS URI ÎòêÎäî Î°úÏª¨ ÌååÏùº Í≤ΩÎ°ú
        if (videoInput.startsWith('gs://')) {
          request = {
            inputUri: videoInput,
            features: this.features,
          };
        } else {
          // Î°úÏª¨ ÌååÏùº Ï≤òÎ¶¨
          const inputContent = readFileSync(videoInput);
          request = {
            inputContent,
            features: this.features,
          };
        }
      } else {
        // Buffer Ï≤òÎ¶¨
        request = {
          inputContent: videoInput,
          features: this.features,
        };
      }

      // ÏùåÏÑ± Ï†ÑÏÇ¨ ÏÑ§Ï†ï
      if (options.enableTranscription || options.enableSpeakerDiarization) {
        request.videoContext = {
          speechTranscriptionConfig: {
            languageCode: 'ko-KR',
            enableSpeakerDiarization: options.enableSpeakerDiarization || false,
            diarizationSpeakerCount: 2,
            enableWordTimeOffsets: true,
            enableWordConfidence: true,
          },
        };
      }

      console.log('üé¨ ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏãúÏûë...');
      
      // Î∂ÑÏÑù ÏöîÏ≤≠ Ïã§Ìñâ
      const [operation] = await this.client.annotateVideo(request);
      
      console.log('‚è≥ Î∂ÑÏÑù Ï≤òÎ¶¨ Ï§ë...');
      const [result] = await operation.promise();
      
      console.log('‚úÖ ÎπÑÎîîÏò§ Î∂ÑÏÑù ÏôÑÎ£å!');
      
      return this.processResults(result);
      
    } catch (error) {
      console.error('‚ùå ÎπÑÎîîÏò§ Î∂ÑÏÑù Ï§ë Ïò§Î•ò:', error);
      throw new Error(`ÎπÑÎîîÏò§ Î∂ÑÏÑù Ïã§Ìå®: ${error instanceof Error ? error.message : 'Ïïå Ïàò ÏóÜÎäî Ïò§Î•ò'}`);
    }
  }

  /**
   * Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Íµ¨Ï°∞ÌôîÎêú ÌòïÌÉúÎ°ú Î≥ÄÌôò
   */
  private processResults(result: any): VideoIntelligenceResults {
    // üîç ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ Î∂ÑÏÑù
    const rawDataSize = JSON.stringify(result).length;
    console.log(`üìä Video Intelligence API Raw Data Size: ${(rawDataSize / 1024 / 1024).toFixed(2)}MB`);
    
    console.log('üîç Video Intelligence API Raw Result:', JSON.stringify({
      hasAnnotationResults: !!result.annotationResults,
      annotationResultsLength: result.annotationResults?.length || 0,
      annotationResultsKeys: result.annotationResults?.[0] ? Object.keys(result.annotationResults[0]) : []
    }, null, 2));

    const annotationResults = result.annotationResults?.[0];
    
    if (!annotationResults) {
      console.warn('‚ö†Ô∏è No annotation results found in API response');
      throw new Error('Î∂ÑÏÑù Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.');
    }

    console.log('üìä Annotation Results Keys:', Object.keys(annotationResults));
    
    // üîç Í∞Å ÌïÑÎìúÎ≥Ñ ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ Ï∏°Ï†ï
    const fieldSizes = {};
    for (const [key, value] of Object.entries(annotationResults)) {
      if (value) {
        const size = JSON.stringify(value).length;
        fieldSizes[key] = `${(size / 1024).toFixed(1)}KB`;
      }
    }
    console.log('üìä Raw Data Field Sizes:', fieldSizes);

    console.log('üìä Detection Counts:', {
      objectAnnotations: annotationResults.objectAnnotations?.length || 0,
      personDetectionAnnotations: annotationResults.personDetectionAnnotations?.length || 0,
      faceDetectionAnnotations: annotationResults.faceDetectionAnnotations?.length || 0,
      speechTranscriptions: annotationResults.speechTranscriptions?.length || 0,
      shotAnnotations: annotationResults.shotAnnotations?.length || 0,
      segmentLabelAnnotations: annotationResults.segmentLabelAnnotations?.length || 0,
      frameLabelAnnotations: annotationResults.frameLabelAnnotations?.length || 0
    });

    // üîç Í∞êÏßÄÎêú Í∞ùÏ≤¥ Ï†ïÎ≥¥ Ï∂úÎ†• (ÎîîÎ≤ÑÍπÖÏö©)
    if (annotationResults.objectAnnotations?.length > 0) {
      console.log('üì¶ Detected Objects:', 
        annotationResults.objectAnnotations.slice(0, 5).map((obj: any) => ({
          description: obj.entity?.description,
          confidence: obj.confidence,
          frameCount: obj.frames?.length || 0,
          segmentDuration: obj.segment ? 
            `${this.parseTimeOffset(obj.segment.startTimeOffset)}s - ${this.parseTimeOffset(obj.segment.endTimeOffset)}s` : 'N/A'
        }))
      );
      
      // Ï†ÑÏ≤¥ Í∞ùÏ≤¥ Î™©Î°ù (ÏöîÏïΩ)
      const objectSummary = annotationResults.objectAnnotations.reduce((acc: any, obj: any) => {
        const desc = obj.entity?.description || 'unknown';
        if (!acc[desc]) {
          acc[desc] = 0;
        }
        acc[desc]++;
        return acc;
      }, {});
      console.log('üì¶ Object Detection Summary:', objectSummary);
    }

    // üîç Segment Labels Î∂ÑÏÑù
    if (annotationResults.segmentLabelAnnotations?.length > 0) {
      console.log('üé¨ Segment Labels (first 10):', 
        annotationResults.segmentLabelAnnotations.slice(0, 10).map((label: any) => ({
          description: label.entity?.description,
          confidence: label.categoryEntities?.[0]?.description,
          segmentCount: label.segments?.length || 0
        }))
      );
    }

    // üîç Frame Labels Î∂ÑÏÑù (Ïù¥Í≤å Ïö©ÎüâÏù¥ ÌÅ¥ Ïàò ÏûàÏùå)
    if (annotationResults.frameLabelAnnotations?.length > 0) {
      const frameLabelSize = JSON.stringify(annotationResults.frameLabelAnnotations).length;
      console.log(`üñºÔ∏è Frame Labels: ${annotationResults.frameLabelAnnotations.length} labels, ${(frameLabelSize / 1024 / 1024).toFixed(2)}MB`);
      
      console.log('üñºÔ∏è Frame Labels Sample (first 5):', 
        annotationResults.frameLabelAnnotations.slice(0, 5).map((label: any) => ({
          description: label.entity?.description,
          frameCount: label.frames?.length || 0
        }))
      );
    }

    // Í∞ùÏ≤¥ Ï∂îÏ†Å Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    const objectTracking = annotationResults.objectAnnotations?.map((obj: any) => ({
      entity: {
        entityId: obj.entity?.entityId || '',
        description: obj.entity?.description || '',
        languageCode: obj.entity?.languageCode || 'ko'
      },
      confidence: obj.confidence || 0,
      frames: obj.frames?.map((frame: any) => ({
        normalizedBoundingBox: {
          left: frame.normalizedBoundingBox?.left || 0,
          top: frame.normalizedBoundingBox?.top || 0,
          right: frame.normalizedBoundingBox?.right || 0,
          bottom: frame.normalizedBoundingBox?.bottom || 0,
        },
        timeOffset: this.parseTimeOffset(frame.timeOffset)
      })) || [],
      segment: {
        startTimeOffset: this.parseTimeOffset(obj.segment?.startTimeOffset),
        endTimeOffset: this.parseTimeOffset(obj.segment?.endTimeOffset)
      }
    })) || [];

    // ÏùåÏÑ± Ï†ÑÏÇ¨ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    const speechTranscription = annotationResults.speechTranscriptions?.map((speech: any) => ({
      alternatives: speech.alternatives?.map((alt: any) => ({
        transcript: alt.transcript || '',
        confidence: alt.confidence || 0,
        words: alt.words?.map((word: any) => ({
          word: word.word || '',
          startTime: this.parseTimeOffset(word.startTime),
          endTime: this.parseTimeOffset(word.endTime),
          confidence: word.confidence || 0,
          speakerTag: word.speakerTag || 0
        })) || []
      })) || [],
      languageCode: speech.languageCode || 'ko'
    })) || [];

    // ÏñºÍµ¥ Í∞êÏßÄ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    const faceDetection = annotationResults.faceAnnotations?.map((face: any) => ({
      tracks: face.tracks?.map((track: any) => ({
        segment: {
          startTimeOffset: this.parseTimeOffset(track.segment?.startTimeOffset),
          endTimeOffset: this.parseTimeOffset(track.segment?.endTimeOffset)
        },
        timestampedObjects: track.timestampedObjects?.map((obj: any) => ({
          normalizedBoundingBox: {
            left: obj.normalizedBoundingBox?.left || 0,
            top: obj.normalizedBoundingBox?.top || 0,
            right: obj.normalizedBoundingBox?.right || 0,
            bottom: obj.normalizedBoundingBox?.bottom || 0,
          },
          timeOffset: this.parseTimeOffset(obj.timeOffset),
          attributes: obj.attributes || [],
          landmarks: obj.landmarks || []
        })) || []
      })) || []
    })) || [];

    // ÏÇ¨Îûå Í∞êÏßÄ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    console.log('üîç Person Detection Raw Data:', JSON.stringify({
      hasPersonDetectionAnnotations: !!annotationResults.personDetectionAnnotations,
      personDetectionLength: annotationResults.personDetectionAnnotations?.length || 0,
      firstPersonSample: annotationResults.personDetectionAnnotations?.[0] || null
    }, null, 2));
    
    const personDetection = annotationResults.personDetectionAnnotations?.map((person: any) => ({
      tracks: person.tracks?.map((track: any) => ({
        segment: {
          startTimeOffset: this.parseTimeOffset(track.segment?.startTimeOffset),
          endTimeOffset: this.parseTimeOffset(track.segment?.endTimeOffset)
        },
        timestampedObjects: track.timestampedObjects?.map((obj: any) => ({
          normalizedBoundingBox: {
            left: obj.normalizedBoundingBox?.left || 0,
            top: obj.normalizedBoundingBox?.top || 0,
            right: obj.normalizedBoundingBox?.right || 0,
            bottom: obj.normalizedBoundingBox?.bottom || 0,
          },
          timeOffset: this.parseTimeOffset(obj.timeOffset),
          attributes: obj.attributes || [],
          landmarks: obj.landmarks || []
        })) || []
      })) || []
    })) || [];

    // Ïû•Î©¥ Î≥ÄÌôî Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    const shotChanges = annotationResults.shotAnnotations?.map((shot: any) => ({
      startTimeOffset: this.parseTimeOffset(shot.startTimeOffset),
      endTimeOffset: this.parseTimeOffset(shot.endTimeOffset)
    })) || [];

    // Î™ÖÏãúÏ†Å ÏΩòÌÖêÏ∏† Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    const explicitContent = annotationResults.explicitAnnotation?.frames?.map((frame: any) => ({
      timeOffset: this.parseTimeOffset(frame.timeOffset),
      pornographyLikelihood: frame.pornographyLikelihood || 'VERY_UNLIKELY'
    })) || [];

    // ÌÖçÏä§Ìä∏ Í∞êÏßÄ Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨
    const textDetection = annotationResults.textAnnotations?.map((text: any) => ({
      text: text.text || '',
      segments: text.segments?.map((segment: any) => ({
        startTimeOffset: this.parseTimeOffset(segment.startTimeOffset),
        endTimeOffset: this.parseTimeOffset(segment.endTimeOffset),
        confidence: segment.confidence || 0,
        words: segment.words?.map((word: any) => ({
          word: word.word || '',
          startTimeOffset: this.parseTimeOffset(word.startTimeOffset),
          endTimeOffset: this.parseTimeOffset(word.endTimeOffset),
          confidence: word.confidence || 0
        })) || []
      })) || []
    })) || [];

    return {
      objectTracking,
      speechTranscription,
      faceDetection,
      personDetection,
      shotChanges,
      explicitContent,
      textDetection
    };

    // üîç Ï≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ ÌÅ¨Í∏∞ Î∂ÑÏÑù
    const processedData = {
      objectTracking,
      speechTranscription,
      faceDetection,
      personDetection,
      shotChanges,
      textDetection
    };

    const processedDataSize = JSON.stringify(processedData).length;
    console.log(`üìä Processed Data Size: ${(processedDataSize / 1024 / 1024).toFixed(2)}MB`);
    console.log(`üìä Data Compression Ratio: ${((rawDataSize - processedDataSize) / rawDataSize * 100).toFixed(1)}% reduced`);
    
    // üîç Ï≤òÎ¶¨Îêú Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞ ÏöîÏïΩ
    console.log('üìä Processed Data Summary:', {
      objectTracking: `${objectTracking.length} objects`,
      speechTranscription: `${speechTranscription.length} segments`,
      faceDetection: `${faceDetection.length} faces`,
      personDetection: `${personDetection.length} persons`,
      shotChanges: `${shotChanges.length} shots`,
      textDetection: `${textDetection.length} texts`
    });

    return processedData;
  }

  /**
   * ÏãúÍ∞Ñ Ïò§ÌîÑÏÖã ÌååÏã±
   */
  private parseTimeOffset(timeOffset: any): number {
    if (!timeOffset) {
      return 0;
    }
    
    const seconds = parseInt(timeOffset.seconds || '0');
    const nanos = parseInt(timeOffset.nanos || '0');
    
    return seconds + nanos / 1000000000;
  }

  /**
   * ÎπÑÎîîÏò§ ÌíàÏßà Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
   */
  async calculateQualityMetrics(analysisResults: VideoIntelligenceResults): Promise<{
    videoQuality: number;
    audioQuality: number;
    overallQuality: number;
  }> {
    // ÎπÑÎîîÏò§ ÌíàÏßà ÌèâÍ∞Ä (Í∞ùÏ≤¥ Ï∂îÏ†Å Î∞è ÏñºÍµ¥ Í∞êÏßÄ Í∏∞Î∞ò)
    const videoQuality = this.assessVideoQuality(analysisResults);
    
    // Ïò§ÎîîÏò§ ÌíàÏßà ÌèâÍ∞Ä (ÏùåÏÑ± Ï†ÑÏÇ¨ Ï†ïÌôïÎèÑ Í∏∞Î∞ò)
    const audioQuality = this.assessAudioQuality(analysisResults);
    
    // Ï†ÑÏ≤¥ ÌíàÏßà Ï†êÏàò
    const overallQuality = (videoQuality + audioQuality) / 2;
    
    return {
      videoQuality: Math.round(videoQuality * 100) / 100,
      audioQuality: Math.round(audioQuality * 100) / 100,
      overallQuality: Math.round(overallQuality * 100) / 100
    };
  }

  private assessVideoQuality(results: VideoIntelligenceResults): number {
    let quality = 0.5; // Í∏∞Î≥∏ Ï†êÏàò

    // Í∞ùÏ≤¥ Ï∂îÏ†Å Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÌèâÍ∞Ä
    if (results.objectTracking?.length > 0) {
      const avgConfidence = results.objectTracking.reduce((sum, obj) => sum + obj.confidence, 0) / results.objectTracking.length;
      quality += avgConfidence * 0.3;
    }

    // ÏñºÍµ¥ Í∞êÏßÄ Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÌèâÍ∞Ä
    if (results.faceDetection?.length > 0) {
      quality += 0.2; // ÏñºÍµ¥Ïù¥ Í∞êÏßÄÎêòÎ©¥ ÌíàÏßà Ìñ•ÏÉÅ
    }

    // ÏÇ¨Îûå Í∞êÏßÄ Îç∞Ïù¥ÌÑ∞ ÌíàÏßà ÌèâÍ∞Ä
    if (results.personDetection?.length > 0) {
      quality += 0.2; // ÏÇ¨ÎûåÏù¥ Í∞êÏßÄÎêòÎ©¥ ÌíàÏßà Ìñ•ÏÉÅ
    }

    return Math.min(quality, 1.0); // ÏµúÎåÄ 1.0ÏúºÎ°ú Ï†úÌïú
  }

  private assessAudioQuality(results: VideoIntelligenceResults): number {
    let quality = 0.5; // Í∏∞Î≥∏ Ï†êÏàò

    // ÏùåÏÑ± Ï†ÑÏÇ¨ ÌíàÏßà ÌèâÍ∞Ä
    if (results.speechTranscription?.length > 0) {
      const transcriptions = results.speechTranscription;
      let totalConfidence = 0;
      let totalWords = 0;

      transcriptions.forEach(speech => {
        speech.alternatives?.forEach(alt => {
          if (alt.words) {
            alt.words.forEach(word => {
              totalConfidence += word.confidence;
              totalWords++;
            });
          }
        });
      });

      if (totalWords > 0) {
        const avgConfidence = totalConfidence / totalWords;
        quality += avgConfidence * 0.5;
      }
    }

    return Math.min(quality, 1.0); // ÏµúÎåÄ 1.0ÏúºÎ°ú Ï†úÌïú
  }

  /**
   * Î∂ÑÏÑù ÏßÑÌñâ ÏÉÅÌô© ÏΩúÎ∞±Í≥º Ìï®Íªò ÎπÑÎîîÏò§ Î∂ÑÏÑù (Ïã§ÏãúÍ∞Ñ ÏóÖÎç∞Ïù¥Ìä∏Ïö©)
   */
  async analyzeVideoWithProgress(
    videoInput: string | Buffer,
    options: VideoAnalysisOptions = {},
    progressCallback?: (progress: number, stage: string) => void
  ): Promise<VideoIntelligenceResults> {
    
    if (progressCallback) {
      progressCallback(10, 'ÎπÑÎîîÏò§ ÏóÖÎ°úÎìú Ï§ë...');
    }

    try {
      // Î∂ÑÏÑù ÏöîÏ≤≠ ÏãúÏûë
      if (progressCallback) {
        progressCallback(30, 'Î∂ÑÏÑù ÏöîÏ≤≠ Ï†ÑÏÜ° Ï§ë...');
      }

      const results = await this.analyzeVideo(videoInput, options);

      if (progressCallback) {
        progressCallback(80, 'Í≤∞Í≥º Ï≤òÎ¶¨ Ï§ë...');
      }

      // ÌíàÏßà Î©îÌä∏Î¶≠ Í≥ÑÏÇ∞
      const qualityMetrics = await this.calculateQualityMetrics(results);

      if (progressCallback) {
        progressCallback(100, 'Î∂ÑÏÑù ÏôÑÎ£å!');
      }

      return results;
    } catch (error) {
      if (progressCallback) {
        progressCallback(0, 'Î∂ÑÏÑù Ïã§Ìå®');
      }
      throw error;
    }
  }
} 