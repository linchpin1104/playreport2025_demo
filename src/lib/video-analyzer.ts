import { readFileSync } from 'fs';
import { VideoIntelligenceServiceClient, protos } from '@google-cloud/video-intelligence';
import config from '@/lib/config';
import { VideoIntelligenceResults } from '@/types';

export interface VideoAnalysisOptions {
  enableVoiceAnalysis?: boolean;
  enableGestureRecognition?: boolean;
  enableObjectDetection?: boolean;
  enableFaceDetection?: boolean;
  enableTranscription?: boolean;
  enableSpeakerDiarization?: boolean;
  enableSentimentAnalysis?: boolean;
  enableQualityMetrics?: boolean;
  enableComprehensiveAnalysis?: boolean;
}

export class VideoAnalyzer {
  private readonly client: VideoIntelligenceServiceClient;
  private readonly features: protos.google.cloud.videointelligence.v1.Feature[];

  constructor() {
    this.client = new VideoIntelligenceServiceClient({
      projectId: config.googleCloud.projectId,
      keyFilename: config.googleCloud.keyFile,
    });

    this.features = [
      protos.google.cloud.videointelligence.v1.Feature.OBJECT_TRACKING,
      protos.google.cloud.videointelligence.v1.Feature.FACE_DETECTION,
      protos.google.cloud.videointelligence.v1.Feature.PERSON_DETECTION,
      protos.google.cloud.videointelligence.v1.Feature.SHOT_CHANGE_DETECTION,
      protos.google.cloud.videointelligence.v1.Feature.SPEECH_TRANSCRIPTION,
      protos.google.cloud.videointelligence.v1.Feature.TEXT_DETECTION,
    ];
  }

  /**
   * ë¹„ë””ì˜¤ ë¶„ì„ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
   */
  async analyzeVideo(
    videoInput: string | Buffer, 
    options: VideoAnalysisOptions = {}
  ): Promise<VideoIntelligenceResults> {
    try {
      let request: any;

      // ì…ë ¥ íƒ€ì…ì— ë”°ë¼ ìš”ì²­ êµ¬ì„±
      if (typeof videoInput === 'string') {
        // GCS URI ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
        if (videoInput.startsWith('gs://')) {
          request = {
            inputUri: videoInput,
            features: this.features,
          };
        } else {
          // ë¡œì»¬ íŒŒì¼ ì²˜ë¦¬
          const inputContent = readFileSync(videoInput);
          request = {
            inputContent,
            features: this.features,
          };
        }
      } else {
        // Buffer ì²˜ë¦¬
        request = {
          inputContent: videoInput,
          features: this.features,
        };
      }

      // ìŒì„± ì „ì‚¬ ì„¤ì •
      if (options.enableTranscription || options.enableSpeakerDiarization) {
        request.videoContext = {
          speechTranscriptionConfig: {
            languageCode: 'ko-KR',
            enableSpeakerDiarization: options.enableSpeakerDiarization || false,
            diarizationSpeakerCount: 2,
            enableWordTimeOffsets: true,
            enableWordConfidence: true,
          },
        };
      }

      console.log('ğŸ¬ ë¹„ë””ì˜¤ ë¶„ì„ ì‹œì‘...');
      
      // ë¶„ì„ ìš”ì²­ ì‹¤í–‰
      const [operation] = await this.client.annotateVideo(request);
      
      console.log('â³ ë¶„ì„ ì²˜ë¦¬ ì¤‘...');
      const [result] = await operation.promise();
      
      console.log('âœ… ë¹„ë””ì˜¤ ë¶„ì„ ì™„ë£Œ!');
      
      return this.processResults(result);
      
    } catch (error) {
      console.error('âŒ ë¹„ë””ì˜¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜:', error);
      throw new Error(`ë¹„ë””ì˜¤ ë¶„ì„ ì‹¤íŒ¨: ${error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'}`);
    }
  }

  /**
   * ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
   */
  private processResults(result: any): VideoIntelligenceResults {
    // ğŸ” ì›ë³¸ ë°ì´í„° í¬ê¸° ë¶„ì„
    const rawDataSize = JSON.stringify(result).length;
    console.log(`ğŸ“Š Video Intelligence API Raw Data Size: ${(rawDataSize / 1024 / 1024).toFixed(2)}MB`);
    
    console.log('ğŸ” Video Intelligence API Raw Result:', JSON.stringify({
      hasAnnotationResults: !!result.annotationResults,
      annotationResultsLength: result.annotationResults?.length || 0,
      annotationResultsKeys: result.annotationResults?.[0] ? Object.keys(result.annotationResults[0]) : []
    }, null, 2));

    const annotationResults = result.annotationResults?.[0];
    
    if (!annotationResults) {
      console.warn('âš ï¸ No annotation results found in API response');
      throw new Error('ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.');
    }

    console.log('ğŸ“Š Annotation Results Keys:', Object.keys(annotationResults));
    
    // ğŸ” ê° í•„ë“œë³„ ì›ë³¸ ë°ì´í„° í¬ê¸° ì¸¡ì •
    const fieldSizes = {};
    for (const [key, value] of Object.entries(annotationResults)) {
      if (value) {
        const size = JSON.stringify(value).length;
        fieldSizes[key] = `${(size / 1024).toFixed(1)}KB`;
      }
    }
    console.log('ğŸ“Š Raw Data Field Sizes:', fieldSizes);

    console.log('ğŸ“Š Detection Counts:', {
      objectAnnotations: annotationResults.objectAnnotations?.length || 0,
      personDetectionAnnotations: annotationResults.personDetectionAnnotations?.length || 0,
      faceDetectionAnnotations: annotationResults.faceDetectionAnnotations?.length || 0,
      speechTranscriptions: annotationResults.speechTranscriptions?.length || 0,
      shotAnnotations: annotationResults.shotAnnotations?.length || 0,
      segmentLabelAnnotations: annotationResults.segmentLabelAnnotations?.length || 0,
      frameLabelAnnotations: annotationResults.frameLabelAnnotations?.length || 0
    });

    // ğŸ” ê°ì§€ëœ ê°ì²´ ì •ë³´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
    if (annotationResults.objectAnnotations?.length > 0) {
      console.log('ğŸ“¦ Detected Objects:', 
        annotationResults.objectAnnotations.slice(0, 5).map((obj: any) => ({
          description: obj.entity?.description,
          confidence: obj.confidence,
          frameCount: obj.frames?.length || 0,
          segmentDuration: obj.segment ? 
            `${this.parseTimeOffset(obj.segment.startTimeOffset)}s - ${this.parseTimeOffset(obj.segment.endTimeOffset)}s` : 'N/A'
        }))
      );
      
      // ì „ì²´ ê°ì²´ ëª©ë¡ (ìš”ì•½)
      const objectSummary = annotationResults.objectAnnotations.reduce((acc: any, obj: any) => {
        const desc = obj.entity?.description || 'unknown';
        if (!acc[desc]) {
          acc[desc] = 0;
        }
        acc[desc]++;
        return acc;
      }, {});
      console.log('ğŸ“¦ Object Detection Summary:', objectSummary);
    }

    // ğŸ” Segment Labels ë¶„ì„
    if (annotationResults.segmentLabelAnnotations?.length > 0) {
      console.log('ğŸ¬ Segment Labels (first 10):', 
        annotationResults.segmentLabelAnnotations.slice(0, 10).map((label: any) => ({
          description: label.entity?.description,
          confidence: label.categoryEntities?.[0]?.description,
          segmentCount: label.segments?.length || 0
        }))
      );
    }

    // ğŸ” Frame Labels ë¶„ì„ (ì´ê²Œ ìš©ëŸ‰ì´ í´ ìˆ˜ ìˆìŒ)
    if (annotationResults.frameLabelAnnotations?.length > 0) {
      const frameLabelSize = JSON.stringify(annotationResults.frameLabelAnnotations).length;
      console.log(`ğŸ–¼ï¸ Frame Labels: ${annotationResults.frameLabelAnnotations.length} labels, ${(frameLabelSize / 1024 / 1024).toFixed(2)}MB`);
      
      console.log('ğŸ–¼ï¸ Frame Labels Sample (first 5):', 
        annotationResults.frameLabelAnnotations.slice(0, 5).map((label: any) => ({
          description: label.entity?.description,
          frameCount: label.frames?.length || 0
        }))
      );
    }

    // ê°ì²´ ì¶”ì  ë°ì´í„° ì²˜ë¦¬
    const objectTracking = annotationResults.objectAnnotations?.map((obj: any) => ({
      entity: {
        entityId: obj.entity?.entityId || '',
        description: obj.entity?.description || '',
        languageCode: obj.entity?.languageCode || 'ko'
      },
      confidence: obj.confidence || 0,
      frames: obj.frames?.map((frame: any) => ({
        normalizedBoundingBox: {
          left: frame.normalizedBoundingBox?.left || 0,
          top: frame.normalizedBoundingBox?.top || 0,
          right: frame.normalizedBoundingBox?.right || 0,
          bottom: frame.normalizedBoundingBox?.bottom || 0,
        },
        timeOffset: this.parseTimeOffset(frame.timeOffset)
      })) || [],
      segment: {
        startTimeOffset: this.parseTimeOffset(obj.segment?.startTimeOffset),
        endTimeOffset: this.parseTimeOffset(obj.segment?.endTimeOffset)
      }
    })) || [];

    // ìŒì„± ì „ì‚¬ ë°ì´í„° ì²˜ë¦¬
    const speechTranscription = annotationResults.speechTranscriptions?.map((speech: any) => ({
      alternatives: speech.alternatives?.map((alt: any) => ({
        transcript: alt.transcript || '',
        confidence: alt.confidence || 0,
        words: alt.words?.map((word: any) => ({
          word: word.word || '',
          startTime: this.parseTimeOffset(word.startTime),
          endTime: this.parseTimeOffset(word.endTime),
          confidence: word.confidence || 0,
          speakerTag: word.speakerTag || 0
        })) || []
      })) || [],
      languageCode: speech.languageCode || 'ko'
    })) || [];

    // ì–¼êµ´ ê°ì§€ ë°ì´í„° ì²˜ë¦¬
    const faceDetection = annotationResults.faceAnnotations?.map((face: any) => ({
      tracks: face.tracks?.map((track: any) => ({
        segment: {
          startTimeOffset: this.parseTimeOffset(track.segment?.startTimeOffset),
          endTimeOffset: this.parseTimeOffset(track.segment?.endTimeOffset)
        },
        timestampedObjects: track.timestampedObjects?.map((obj: any) => ({
          normalizedBoundingBox: {
            left: obj.normalizedBoundingBox?.left || 0,
            top: obj.normalizedBoundingBox?.top || 0,
            right: obj.normalizedBoundingBox?.right || 0,
            bottom: obj.normalizedBoundingBox?.bottom || 0,
          },
          timeOffset: this.parseTimeOffset(obj.timeOffset),
          attributes: obj.attributes || [],
          landmarks: obj.landmarks || []
        })) || []
      })) || []
    })) || [];

    // ì‚¬ëŒ ê°ì§€ ë°ì´í„° ì²˜ë¦¬
    console.log('ğŸ” Person Detection Raw Data:', JSON.stringify({
      hasPersonDetectionAnnotations: !!annotationResults.personDetectionAnnotations,
      personDetectionLength: annotationResults.personDetectionAnnotations?.length || 0,
      firstPersonSample: annotationResults.personDetectionAnnotations?.[0] || null
    }, null, 2));
    
    const personDetection = annotationResults.personDetectionAnnotations?.map((person: any) => ({
      tracks: person.tracks?.map((track: any) => ({
        segment: {
          startTimeOffset: this.parseTimeOffset(track.segment?.startTimeOffset),
          endTimeOffset: this.parseTimeOffset(track.segment?.endTimeOffset)
        },
        timestampedObjects: track.timestampedObjects?.map((obj: any) => ({
          normalizedBoundingBox: {
            left: obj.normalizedBoundingBox?.left || 0,
            top: obj.normalizedBoundingBox?.top || 0,
            right: obj.normalizedBoundingBox?.right || 0,
            bottom: obj.normalizedBoundingBox?.bottom || 0,
          },
          timeOffset: this.parseTimeOffset(obj.timeOffset),
          attributes: obj.attributes || [],
          landmarks: obj.landmarks || []
        })) || []
      })) || []
    })) || [];

    // ì¥ë©´ ë³€í™” ë°ì´í„° ì²˜ë¦¬
    const shotChanges = annotationResults.shotAnnotations?.map((shot: any) => ({
      startTimeOffset: this.parseTimeOffset(shot.startTimeOffset),
      endTimeOffset: this.parseTimeOffset(shot.endTimeOffset)
    })) || [];

    // ëª…ì‹œì  ì½˜í…ì¸  ë°ì´í„° ì²˜ë¦¬
    const explicitContent = annotationResults.explicitAnnotation?.frames?.map((frame: any) => ({
      timeOffset: this.parseTimeOffset(frame.timeOffset),
      pornographyLikelihood: frame.pornographyLikelihood || 'VERY_UNLIKELY'
    })) || [];

    // í…ìŠ¤íŠ¸ ê°ì§€ ë°ì´í„° ì²˜ë¦¬
    const textDetection = annotationResults.textAnnotations?.map((text: any) => ({
      text: text.text || '',
      segments: text.segments?.map((segment: any) => ({
        startTimeOffset: this.parseTimeOffset(segment.startTimeOffset),
        endTimeOffset: this.parseTimeOffset(segment.endTimeOffset),
        confidence: segment.confidence || 0,
        words: segment.words?.map((word: any) => ({
          word: word.word || '',
          startTimeOffset: this.parseTimeOffset(word.startTimeOffset),
          endTimeOffset: this.parseTimeOffset(word.endTimeOffset),
          confidence: word.confidence || 0
        })) || []
      })) || []
    })) || [];

    return {
      objectTracking,
      speechTranscription,
      faceDetection,
      personDetection,
      shotChanges,
      explicitContent,
      textDetection
    };

    // ğŸ” ì²˜ë¦¬ëœ ë°ì´í„° í¬ê¸° ë¶„ì„
    const processedData = {
      objectTracking,
      speechTranscription,
      faceDetection,
      personDetection,
      shotChanges,
      textDetection
    };

    const processedDataSize = JSON.stringify(processedData).length;
    console.log(`ğŸ“Š Processed Data Size: ${(processedDataSize / 1024 / 1024).toFixed(2)}MB`);
    console.log(`ğŸ“Š Data Compression Ratio: ${((rawDataSize - processedDataSize) / rawDataSize * 100).toFixed(1)}% reduced`);
    
    // ğŸ” ì²˜ë¦¬ëœ ë°ì´í„° êµ¬ì¡° ìš”ì•½
    console.log('ğŸ“Š Processed Data Summary:', {
      objectTracking: `${objectTracking.length} objects`,
      speechTranscription: `${speechTranscription.length} segments`,
      faceDetection: `${faceDetection.length} faces`,
      personDetection: `${personDetection.length} persons`,
      shotChanges: `${shotChanges.length} shots`,
      textDetection: `${textDetection.length} texts`
    });

    return processedData;
  }

  /**
   * ì‹œê°„ ì˜¤í”„ì…‹ íŒŒì‹±
   */
  private parseTimeOffset(timeOffset: any): number {
    if (!timeOffset) {
      return 0;
    }
    
    const seconds = parseInt(timeOffset.seconds || '0');
    const nanos = parseInt(timeOffset.nanos || '0');
    
    return seconds + nanos / 1000000000;
  }

  /**
   * ë¹„ë””ì˜¤ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
   */
  async calculateQualityMetrics(analysisResults: VideoIntelligenceResults): Promise<{
    videoQuality: number;
    audioQuality: number;
    overallQuality: number;
  }> {
    // ë¹„ë””ì˜¤ í’ˆì§ˆ í‰ê°€ (ê°ì²´ ì¶”ì  ë° ì–¼êµ´ ê°ì§€ ê¸°ë°˜)
    const videoQuality = this.assessVideoQuality(analysisResults);
    
    // ì˜¤ë””ì˜¤ í’ˆì§ˆ í‰ê°€ (ìŒì„± ì „ì‚¬ ì •í™•ë„ ê¸°ë°˜)
    const audioQuality = this.assessAudioQuality(analysisResults);
    
    // ì „ì²´ í’ˆì§ˆ ì ìˆ˜
    const overallQuality = (videoQuality + audioQuality) / 2;
    
    return {
      videoQuality: Math.round(videoQuality * 100) / 100,
      audioQuality: Math.round(audioQuality * 100) / 100,
      overallQuality: Math.round(overallQuality * 100) / 100
    };
  }

  private assessVideoQuality(results: VideoIntelligenceResults): number {
    let quality = 0.5; // ê¸°ë³¸ ì ìˆ˜

    // ê°ì²´ ì¶”ì  ë°ì´í„° í’ˆì§ˆ í‰ê°€
    if (results.objectTracking?.length > 0) {
      const avgConfidence = results.objectTracking.reduce((sum, obj) => sum + obj.confidence, 0) / results.objectTracking.length;
      quality += avgConfidence * 0.3;
    }

    // ì–¼êµ´ ê°ì§€ ë°ì´í„° í’ˆì§ˆ í‰ê°€
    if (results.faceDetection?.length > 0) {
      quality += 0.2; // ì–¼êµ´ì´ ê°ì§€ë˜ë©´ í’ˆì§ˆ í–¥ìƒ
    }

    // ì‚¬ëŒ ê°ì§€ ë°ì´í„° í’ˆì§ˆ í‰ê°€
    if (results.personDetection?.length > 0) {
      quality += 0.2; // ì‚¬ëŒì´ ê°ì§€ë˜ë©´ í’ˆì§ˆ í–¥ìƒ
    }

    return Math.min(quality, 1.0); // ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
  }

  private assessAudioQuality(results: VideoIntelligenceResults): number {
    let quality = 0.5; // ê¸°ë³¸ ì ìˆ˜

    // ìŒì„± ì „ì‚¬ í’ˆì§ˆ í‰ê°€
    if (results.speechTranscription?.length > 0) {
      const transcriptions = results.speechTranscription;
      let totalConfidence = 0;
      let totalWords = 0;

      transcriptions.forEach(speech => {
        speech.alternatives?.forEach(alt => {
          if (alt.words) {
            alt.words.forEach(word => {
              totalConfidence += word.confidence;
              totalWords++;
            });
          }
        });
      });

      if (totalWords > 0) {
        const avgConfidence = totalConfidence / totalWords;
        quality += avgConfidence * 0.5;
      }
    }

    return Math.min(quality, 1.0); // ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
  }

  /**
   * ë¶„ì„ ì§„í–‰ ìƒí™© ì½œë°±ê³¼ í•¨ê»˜ ë¹„ë””ì˜¤ ë¶„ì„ (ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ìš©)
   */
  async analyzeVideoWithProgress(
    videoInput: string | Buffer,
    options: VideoAnalysisOptions = {},
    progressCallback?: (progress: number, stage: string) => void
  ): Promise<VideoIntelligenceResults> {
    
    if (progressCallback) {
      progressCallback(10, 'ë¹„ë””ì˜¤ ì—…ë¡œë“œ ì¤‘...');
    }

    try {
      // ë¶„ì„ ìš”ì²­ ì‹œì‘
      if (progressCallback) {
        progressCallback(30, 'ë¶„ì„ ìš”ì²­ ì „ì†¡ ì¤‘...');
      }

      const results = await this.analyzeVideo(videoInput, options);

      if (progressCallback) {
        progressCallback(80, 'ê²°ê³¼ ì²˜ë¦¬ ì¤‘...');
      }

      // í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
      const qualityMetrics = await this.calculateQualityMetrics(results);

      if (progressCallback) {
        progressCallback(100, 'ë¶„ì„ ì™„ë£Œ!');
      }

      return results;
    } catch (error) {
      if (progressCallback) {
        progressCallback(0, 'ë¶„ì„ ì‹¤íŒ¨');
      }
      throw error;
    }
  }
} 