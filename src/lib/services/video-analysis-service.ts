import { PlayAnalysisCore , PlayAnalysisExtractor } from '@/lib/play-analysis-extractor';
import { VideoIntelligenceResults } from '@/types';
import { AppError } from '../errors';
import { GCPDataStorage } from '../gcp-data-storage';
import { IntegratedAnalysisSystem , IntegratedAnalysisResult } from '../integrated-analysis-system';
import { ServiceResult } from '../interfaces';
import { VideoAnalyzer } from '../video-analyzer';
import { ErrorHandlingService } from './error-handling-service';
import { Logger } from './logger';
import { ConfigManager } from './config-manager';

export interface VideoAnalysisRequest {
  sessionId?: string;
  gsUri?: string;
  fileName?: string;
  options?: {
    enableVoiceAnalysis?: boolean;
    enableGestureRecognition?: boolean;
    enableObjectDetection?: boolean;
    enableFaceDetection?: boolean;
    enableTranscription?: boolean;
    enableSpeakerDiarization?: boolean;
    enableSentimentAnalysis?: boolean;
    enableQualityMetrics?: boolean;
    enableComprehensiveAnalysis?: boolean;
  };
  participantInfo?: {
    childAge?: number;
    specialNeeds?: string[];
    previousAnalysis?: string[];
  };
}

export interface VideoAnalysisResult {
  analysisResults: VideoIntelligenceResults;
  metadata: {
    fileName: string;
    gsUri: string;
    processingTime: number;
    analysisMode: 'development' | 'production';
    sessionId?: string;
  };
  stage1Complete: boolean;
  stage2Complete: boolean;
  stage3Complete: boolean;
  stage4Complete: boolean;
}

export interface ProcessedAnalysisData {
  videoAnalysisData: {
    objectTracking: VideoIntelligenceResults['objectTracking'];
    faceDetection: VideoIntelligenceResults['faceDetection'];
    personDetection: VideoIntelligenceResults['personDetection'];
    shotChanges: VideoIntelligenceResults['shotChanges'];
  };
  audioAnalysisData: {
    transcript: TranscriptEntry[];
    speakers: string[];
    emotions: EmotionEntry[];
    voiceMetrics: VoiceMetricsEntry[];
  };
  sessionMetadata: {
    duration: number;
    participants: string[];
    sessionType: 'play-interaction';
    timestamp: string;
  };
}

export interface TranscriptEntry {
  text: string;
  confidence: number;
  startTime: number;
  endTime: number;
  speaker: string;
  time: number; // í˜¸í™˜ì„±ì„ ìœ„í•´ ì¶”ê°€
}

export interface EmotionEntry {
  emotion: string;
  confidence: number;
  timeSegment: { start: number; end: number };
  speaker: string;
}

export interface VoiceMetricsEntry {
  volume: number;
  pitch: number;
  rate: number;
  timeSegment: { start: number; end: number };
  speaker: string;
}

export class VideoAnalysisService {
  private readonly logger: Logger;
  private readonly errorHandler: ErrorHandlingService;

  constructor(
    private readonly videoAnalyzer: VideoAnalyzer,
    private readonly gcpDataStorage: GCPDataStorage,
    private readonly integratedAnalysisSystem: IntegratedAnalysisSystem,
    private readonly playAnalysisExtractor: PlayAnalysisExtractor
  ) {
    this.logger = new Logger('VideoAnalysisService');
    this.errorHandler = new ErrorHandlingService();
  }

  /**
   * ì „ì²´ ë¹„ë””ì˜¤ ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
   */
  async performCompleteAnalysis(request: VideoAnalysisRequest): Promise<ServiceResult<VideoAnalysisResult>> {
    return this.errorHandler.wrapAsync(
      'complete-video-analysis',
      async () => {
        this.logger.info('Starting complete video analysis', { request });

        // ëŸ°íƒ€ì„ì—ì„œ í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜ ê²€ì¦
        try {
          const configManager = ConfigManager.getInstance();
          
          // GCP ê´€ë ¨ ì„¤ì •ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì²´í¬
          if (request.options?.enableTranscription || 
              request.options?.enableSpeakerDiarization || 
              request.options?.enableComprehensiveAnalysis) {
            configManager.validateRequiredConfig(['gcp.keyFile']);
          }
          
          // OpenAI ê´€ë ¨ ì„¤ì •ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì²´í¬
          if (request.options?.enableComprehensiveAnalysis ||
              request.options?.enableSentimentAnalysis) {
            configManager.validateRequiredConfig(['apis.openai.apiKey']);
          }
        } catch (configError) {
          // í™˜ê²½ë³€ìˆ˜ ëˆ„ë½ ì‹œ ê²½ê³  ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰ (ê°œë°œ ëª¨ë“œ)
          this.logger.warn('Configuration warning (continuing with limited features):', {
            error: configError instanceof Error ? configError.message : String(configError)
          });
        }

        // 1. ë¹„ë””ì˜¤ ê²½ë¡œ í™•ì¸
        const videoPathResult = await this.resolveVideoPath(request);
        if (videoPathResult.isFailure()) {
          throw new Error(`Video path resolution failed: ${videoPathResult.getError().message}`);
        }

        const { videoPath, sessionData } = videoPathResult.getValue();

        // 2. Stage 1: ê¸°ë³¸ ë¹„ë””ì˜¤ ë¶„ì„
        this.logger.info('ğŸ¬ Stage 1: Google Cloud Video Intelligence ë¶„ì„ ì‹œì‘...');
        const stage1Result = await this.performStage1Analysis(videoPath, request.options);
        if (stage1Result.isFailure()) {
          throw new Error(`Stage 1 analysis failed: ${stage1Result.getError().message}`);
        }

        const analysisResults = stage1Result.getValue();
        this.logger.info('âœ… Stage 1 ì™„ë£Œ: ê¸°ë³¸ ë¹„ë””ì˜¤ ë¶„ì„ ì™„ë£Œ');

        // 3. Stage 2: ë°ì´í„° ë¶„ë¦¬ ë° ê°€ê³µ
        this.logger.info('ğŸ”„ Stage 2: ë°ì´í„° ë¶„ë¦¬ ë° ê°€ê³µ ì‹œì‘...');
        const stage2Result = await this.performStage2Processing(analysisResults);
        if (stage2Result.isFailure()) {
          throw new Error(`Stage 2 processing failed: ${stage2Result.getError().message}`);
        }

        const processedData = stage2Result.getValue();
        this.logger.info('âœ… Stage 2 ì™„ë£Œ: ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ');

        // 4. Stage 3: í†µí•© ë¶„ì„
        this.logger.info('ğŸ¯ Stage 3: í†µí•© ë¶„ì„ ì‹œì‘...');
        const stage3Result = await this.performStage3Analysis(processedData);
        if (stage3Result.isFailure()) {
          throw new Error(`Stage 3 analysis failed: ${stage3Result.getError().message}`);
        }

        const integratedResults = stage3Result.getValue();
        this.logger.info('âœ… Stage 3 ì™„ë£Œ: í†µí•© ë¶„ì„ ì™„ë£Œ');

        // 5. Stage 4: í•µì‹¬ ì •ë³´ ì¶”ì¶œ (ì›ë³¸ analysisResults ì‚¬ìš©)
        this.logger.info('ğŸª Stage 4: í•µì‹¬ ì •ë³´ ì¶”ì¶œ ì‹œì‘...');
        const stage4Result = await this.performStage4Extraction(analysisResults, sessionData);
        if (stage4Result.isFailure()) {
          throw new Error(`Stage 4 extraction failed: ${stage4Result.getError().message}`);
        }

        this.logger.info('âœ… Stage 4 ì™„ë£Œ: í•µì‹¬ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ');
        this.logger.info('ğŸ‰ ì „ì²´ ë¶„ì„ ì™„ë£Œ!');

        // 6. ê²°ê³¼ êµ¬ì„±
        const result: VideoAnalysisResult = {
          analysisResults,
          metadata: {
            fileName: request.fileName || sessionData?.metadata?.fileName || 'unknown',
            gsUri: videoPath,
            processingTime: Date.now(),
            analysisMode: 'production',
            sessionId: request.sessionId
          },
          stage1Complete: true,
          stage2Complete: true,
          stage3Complete: true,
          stage4Complete: true
        };

        return result;
      },
      {
        sessionId: request.sessionId,
        metadata: {
          fileName: request.fileName,
          gsUri: request.gsUri,
          endpoint: '/api/analyze',
          timestamp: new Date().toISOString()
        }
      }
    );
  }

  /**
   * ë¹„ë””ì˜¤ ê²½ë¡œ í•´ê²°
   */
  private async resolveVideoPath(request: VideoAnalysisRequest): Promise<ServiceResult<{ videoPath: string; sessionData: any }>> {
    return this.errorHandler.wrapAsync(
      'resolve-video-path',
      async () => {
        if (!request.sessionId && !request.gsUri) {
          throw new Error('sessionId ë˜ëŠ” gsUriê°€ í•„ìš”í•©ë‹ˆë‹¤.');
        }

        if (request.sessionId) {
          const session = await this.gcpDataStorage.getSession(request.sessionId);
          if (!session) {
            throw new Error('ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
          }

          const videoPath = session.paths.rawDataPath;
          if (!videoPath) {
            throw new Error('ë¹„ë””ì˜¤ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.');
          }

          return { videoPath, sessionData: session };
        } else {
          return { videoPath: request.gsUri!, sessionData: null };
        }
      },
      {
        sessionId: request.sessionId,
        metadata: {
          gsUri: request.gsUri,
          timestamp: new Date().toISOString()
        }
      }
    );
  }

  /**
   * Stage 1: ê¸°ë³¸ ë¹„ë””ì˜¤ ë¶„ì„
   */
  private async performStage1Analysis(videoPath: string, options: Record<string, any> = {}): Promise<ServiceResult<VideoIntelligenceResults>> {
    return this.errorHandler.wrapAsync(
      'stage1-video-analysis',
      async () => {
        const analysisOptions = {
          enableVoiceAnalysis: true,
          enableGestureRecognition: true,
          enableObjectDetection: true,
          enableFaceDetection: true,
          enableTranscription: true,
          enableSpeakerDiarization: true,
          enableSentimentAnalysis: true,
          enableQualityMetrics: true,
          enableComprehensiveAnalysis: true,
          ...options
        };

        const results = await this.videoAnalyzer.analyzeVideo(videoPath, analysisOptions);
        
        // ğŸš¨ í•µì‹¬: ì‚¬ëŒ ê°ì§€ í™•ì¸ (Person Detection ë˜ëŠ” Object Detection)
        const hasPersonDetection = results.personDetection && results.personDetection.length > 0;
        const hasPersonInObjects = results.objectTracking && 
          results.objectTracking.some((obj: any) => 
            obj.entity?.description?.toLowerCase() === 'person' && obj.confidence > 0.5
          );

        if (!hasPersonDetection && !hasPersonInObjects) {
          throw new Error(
            'ì˜ìƒì—ì„œ ì‚¬ëŒì„ ê°ì§€í•  ìˆ˜ ì—†ì–´ ë†€ì´ ìƒí˜¸ì‘ìš© ë¶„ì„ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ' +
            'ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:\n' +
            'â€¢ ì˜ìƒì— ì‚¬ëŒì´ ëª…í™•í•˜ê²Œ ë³´ì´ëŠ”ì§€ í™•ì¸\n' +
            'â€¢ ì˜ìƒ í™”ì§ˆì´ ì¶©ë¶„í•œì§€ í™•ì¸\n' +
            'â€¢ ì¡°ëª…ì´ ì ì ˆí•œì§€ í™•ì¸\n' +
            'â€¢ ì¹´ë©”ë¼ê°€ ì‚¬ëŒ ì „ì²´ë¥¼ ì´¬ì˜í•˜ê³  ìˆëŠ”ì§€ í™•ì¸'
          );
        }

        // Person Detectionì´ ì‹¤íŒ¨í–ˆì§€ë§Œ Object Detectionì—ì„œ personì„ ì°¾ì€ ê²½ìš° ë¡œê·¸
        if (!hasPersonDetection && hasPersonInObjects) {
          console.log('â„¹ï¸ Person Detection API ì‹¤íŒ¨, Object Detectionì—ì„œ ì‚¬ëŒ ê°ì§€ ëŒ€ì²´ ì‚¬ìš©');
          
          // Object Detection ê²°ê³¼ë¥¼ Person Detection í˜•íƒœë¡œ ë³€í™˜
          const personObjects = results.objectTracking.filter((obj: any) => 
            obj.entity?.description?.toLowerCase() === 'person' && obj.confidence > 0.5
          );
          
          console.log(`ğŸ‘¥ Object Detectionì—ì„œ ê°ì§€ëœ ì‚¬ëŒ: ${personObjects.length}ëª…`);
          personObjects.forEach((person: any, index: number) => {
            console.log(`ğŸ‘¤ Person ${index + 1}: ì‹ ë¢°ë„ ${(person.confidence * 100).toFixed(1)}%, í”„ë ˆì„ ${person.frames?.length || 0}ê°œ`);
          });
        }
        
        // ì–¼êµ´ì´ë‚˜ ìŒì„± ì „ì‚¬ë„ í™•ì¸ (ì„ íƒì‚¬í•­ì´ì§€ë§Œ ê²½ê³ )
        if (results.faceDetection.length === 0 && results.speechTranscription.length === 0) {
          this.logger.warn('âš ï¸ ì–¼êµ´ê³¼ ìŒì„±ì´ ëª¨ë‘ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¶„ì„ í’ˆì§ˆì´ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.');
        }

        return results;
      },
      {
        metadata: {
          videoPath,
          stage: 'stage1',
          timestamp: new Date().toISOString()
        }
      }
    );
  }

  /**
   * Stage 2: ë°ì´í„° ë¶„ë¦¬ ë° ê°€ê³µ
   */
  private async performStage2Processing(analysisResults: VideoIntelligenceResults): Promise<ServiceResult<ProcessedAnalysisData>> {
    return this.errorHandler.wrapAsync(
      'stage2-data-processing',
      async () => {
        // í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ê°€ê³µ
        const processedData: ProcessedAnalysisData = {
          videoAnalysisData: {
            objectTracking: analysisResults.objectTracking || [],
            faceDetection: analysisResults.faceDetection || [],
            personDetection: analysisResults.personDetection || [],
            shotChanges: analysisResults.shotChanges || []
          },
          audioAnalysisData: {
            transcript: this.extractTranscript(analysisResults.speechTranscription),
            speakers: this.extractSpeakers(analysisResults.speechTranscription),
            emotions: [],
            voiceMetrics: []
          },
          sessionMetadata: {
            duration: this.calculateDuration(analysisResults.shotChanges),
            participants: ['parent', 'child'],
            sessionType: 'play-interaction',
            timestamp: new Date().toISOString()
          }
        };

        return processedData;
      },
      {
        metadata: {
          stage: 'stage2',
          dataSize: JSON.stringify(analysisResults).length,
          timestamp: new Date().toISOString()
        }
      }
    );
  }

  /**
   * Stage 3: í†µí•© ë¶„ì„
   */
  private async performStage3Analysis(processedData: ProcessedAnalysisData): Promise<ServiceResult<IntegratedAnalysisResult>> {
    return this.errorHandler.wrapAsync(
      'stage3-integrated-analysis',
      async () => {
        return await this.integratedAnalysisSystem.performIntegratedAnalysis(
          processedData,
          processedData.sessionMetadata.timestamp
        );
      },
      {
        metadata: {
          stage: 'stage3',
          sessionType: processedData.sessionMetadata.sessionType,
          timestamp: new Date().toISOString()
        }
      }
    );
  }

  /**
   * Stage 4: í•µì‹¬ ì •ë³´ ì¶”ì¶œ
   */
  private async performStage4Extraction(analysisResults: VideoIntelligenceResults, sessionData: any): Promise<ServiceResult<PlayAnalysisCore>> {
    return this.errorHandler.wrapAsync(
      'stage4-core-extraction',
      async () => {
        const playCore = await this.playAnalysisExtractor.extractPlayAnalysisCore(
          analysisResults,
          {
            fileName: sessionData?.metadata?.fileName || 'unknown',
            fileSize: sessionData?.metadata?.fileSize || 0,
            exportTime: new Date().toISOString()
          }
        );

        // ì„¸ì…˜ì´ ìˆëŠ” ê²½ìš° í•µì‹¬ ì •ë³´ ì €ì¥
        if (sessionData?.sessionId) {
          await this.gcpDataStorage.savePlayCore(sessionData.sessionId, playCore);
          await this.gcpDataStorage.updateSessionStatus(sessionData.sessionId, 'analyzed');
        }

        return playCore;
      },
      {
        sessionId: sessionData?.sessionId,
        metadata: {
          stage: 'stage4',
          fileName: sessionData?.metadata?.fileName,
          timestamp: new Date().toISOString()
        }
      }
    );
  }

  /**
   * ì „ì‚¬ ë°ì´í„° ì¶”ì¶œ
   */
  private extractTranscript(speechTranscription: any[]): TranscriptEntry[] {
    if (!speechTranscription || speechTranscription.length === 0) {
      return [];
    }

    return speechTranscription.map(item => ({
      text: item.alternatives?.[0]?.transcript || '',
      confidence: item.alternatives?.[0]?.confidence || 0,
      startTime: item.alternatives?.[0]?.words?.[0]?.startTime || 0,
      endTime: item.alternatives?.[0]?.words?.[item.alternatives[0].words.length - 1]?.endTime || 0,
      speaker: item.alternatives?.[0]?.words?.[0]?.speakerTag?.toString() || 'unknown',
      time: item.alternatives?.[0]?.words?.[0]?.startTime || 0
    }));
  }

  /**
   * í™”ì ì •ë³´ ì¶”ì¶œ
   */
  private extractSpeakers(speechTranscription: any[]): string[] {
    if (!speechTranscription || speechTranscription.length === 0) {
      return [];
    }

    const speakers = new Set<string>();
    speechTranscription.forEach(item => {
      item.alternatives?.[0]?.words?.forEach((word: any) => {
        if (word.speakerTag) {
          speakers.add(`speaker_${word.speakerTag}`);
        }
      });
    });

    return Array.from(speakers);
  }

  /**
   * ë¹„ë””ì˜¤ ì§€ì† ì‹œê°„ ê³„ì‚°
   */
  private calculateDuration(shotChanges: any[]): number {
    if (!shotChanges || shotChanges.length === 0) {
      return 0;
    }

    const lastShot = shotChanges[shotChanges.length - 1];
    return lastShot.endTimeOffset || 0;
  }
} 